{
  "metadata": {
    "dataExplorerConfig": {},
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "kernelspec": {
      "display_name": "accelerators",
      "language": "python",
      "name": "bento_kernel_accelerators",
      "metadata": {
        "kernel_name": "bento_kernel_accelerators",
        "nightly_builds": true,
        "fbpkg_supported": true,
        "cinder_runtime": false,
        "is_prebuilt": true
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "last_server_session_id": "c6f6ab3c-9274-41e7-8592-b1b583442e00",
    "last_kernel_id": "fcbf3a69-76a4-4730-9b41-bcd0b24729ca",
    "last_base_url": "https://devgpu005.ftw6.facebook.com:8093/",
    "last_msg_id": "e28f842c-f32dde25c1b80ef7d423dfee_407",
    "outputWidgetContext": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "8ca7695d-8a19-454e-b32b-3d5c36d52faf",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "The purpose of this example is to demostrate the overall flow of lowering a PyTorch model\n",
        "to TensorRT conveniently with lower.py. We integrated the transformation process including `TRTInterpreter`, `TRTModule`, pass optimization into the `lower_to_trt` API, users are encouraged to check the docstring of the API and tune it to meet your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "7909785f-b9b4-41dd-82af-c144b879df39",
        "showInput": true,
        "customInput": null,
        "collapsed": false,
        "requestMsgId": "7db2accc-9fa4-4a1e-8142-d887f2947bcd",
        "customOutput": null,
        "executionStartTime": 1656395936225,
        "executionStopTime": 1656395937851
      },
      "source": [
        "import typing as t\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass, field, replace\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch_tensorrt.fx.lower import lower_to_trt\n",
        "from torch_tensorrt.fx.utils import LowerPrecision"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "e324a1ff-1bc2-4e78-932f-33534c3ac3f5",
        "showInput": false,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "Specify the `configuration` class used for FX path lowering and benchmark. To extend, add a new configuration field to this class, and modify the lowering or benchmark behavior in `run_configuration_benchmark()` correspondingly. It automatically stores all its values to a `Result` dataclass.   \n",
        "`Result` is another dataclass that holds raw essential benchmark result values like Batch size, QPS, accuracy, etc..\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "a4455135-8633-4d2d-bdd3-6435a4a9f4dd",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "2835fffa-cc50-479a-9080-c4f7002c0726",
        "customOutput": null,
        "executionStartTime": 1656398717455,
        "executionStopTime": 1656398717662
      },
      "source": [
        "@dataclass\n",
        "class Configuration:\n",
        "    # number of inferences to run\n",
        "    batch_iter: int\n",
        "\n",
        "    # Input batch size\n",
        "    batch_size: int\n",
        "\n",
        "    # Friendly name of the configuration\n",
        "    name: str = \"\"\n",
        "\n",
        "    # Whether to apply TRT lowering to the model before benchmarking\n",
        "    trt: bool = False\n",
        "\n",
        "    # Whether to apply engine holder to the lowered model\n",
        "    jit: bool = False\n",
        "\n",
        "    # Whether to enable FP16 mode for TRT lowering\n",
        "    fp16: bool = False\n",
        "\n",
        "    # Relative tolerance for accuracy check after lowering. -1 means do not\n",
        "    # check accuracy.\n",
        "    accuracy_rtol: float = -1  # disable\n",
        "    \n",
        "@dataclass\n",
        "class Result:\n",
        "    module: torch.nn.Module = field(repr=False)\n",
        "    input: t.Any = field(repr=False)\n",
        "    conf: Configuration\n",
        "    time_sec: float\n",
        "    accuracy_res: t.Optional[bool] = None\n",
        "\n",
        "    @property\n",
        "    def time_per_iter_ms(self) -> float:\n",
        "        return self.time_sec * 1.0e3\n",
        "\n",
        "    @property\n",
        "    def qps(self) -> float:\n",
        "        return self.conf.batch_size / self.time_sec\n",
        "\n",
        "    def format(self) -> str:\n",
        "        return (\n",
        "            f\"== Benchmark Result for: {self.conf}\\n\"\n",
        "            f\"BS: {self.conf.batch_size}, \"\n",
        "            f\"Time per iter: {self.time_per_iter_ms:.2f}ms, \"\n",
        "            f\"QPS: {self.qps:.2f}, \"\n",
        "            f\"Accuracy: {self.accuracy_res} (rtol={self.conf.accuracy_rtol})\"\n",
        "        )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "3e462cf6-d282-402d-955b-a3ecb400bf0b",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": []
      },
      "source": [
        "Run FX path lowering and benchmark the given model according to the specified benchmark configuration. Prints the benchmark result for each configuration at the end of the run. `benchmark_torch_function` is the actual function that computes the fixed number of iterations of functions runs.\n",
        "The FX path lowering and TensorRT engine creation is integrated into `low_to_trt()` API which is defined in `fx/lower.py` file.\n",
        "It is good to list it out and show the usage of it. It takes in original module, input and lowering setting, run lowering workflow to turn module into a executable TRT engine \n",
        "```\n",
        "def lower_to_trt(\n",
        "    module: nn.Module,\n",
        "    input: ,\n",
        "    max_batch_size: int = 2048,\n",
        "    max_workspace_size=1 << 25,\n",
        "    explicit_batch_dimension=False,\n",
        "    lower_precision=LowerPrecision.FP16,\n",
        "    verbose_log=False,\n",
        "    timing_cache_prefix=\"\",\n",
        "    save_timing_cache=False,\n",
        "    cuda_graph_batch_size=-1,\n",
        "    dynamic_batch=False,\n",
        ") -> nn.Module:\n",
        "``` \n",
        "\n",
        "    Args:\n",
        "        module: Original module for lowering.\n",
        "        input: Input for module.\n",
        "        max_batch_size: Maximum batch size (must be >= 1 to be set, 0 means not set)\n",
        "        max_workspace_size: Maximum size of workspace given to TensorRT.\n",
        "        explicit_batch_dimension: Use explicit batch dimension in TensorRT if set True, otherwise use implicit batch dimension.\n",
        "        lower_precision: lower_precision config given to TRTModule.\n",
        "        verbose_log: Enable verbose log for TensorRT if set True.\n",
        "        timing_cache_prefix: Timing cache file name for timing cache used by fx2trt.\n",
        "        save_timing_cache: Update timing cache with current timing cache data if set to True.\n",
        "        cuda_graph_batch_size: Cuda graph batch size, default to be -1.\n",
        "        dynamic_batch: batch dimension (dim=0) is dynamic.\n",
        "\n",
        "    Returns:\n",
        "        A torch.nn.Module lowered by TensorRT.\n",
        "We testd a resnet18 network with input size of [128,3,224,224] for [Batch, Channel, Width, Height]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "originalKey": "91333212-7f6d-4bde-a248-44d485e83e5e",
        "showInput": true,
        "customInput": null,
        "code_folding": [],
        "hidden_ranges": [],
        "collapsed": false,
        "requestMsgId": "3002935b-b95a-4a08-a57f-f7a35485af5b",
        "customOutput": null,
        "executionStartTime": 1656397903207,
        "executionStopTime": 1656397964752
      },
      "source": [
        "test_model = torchvision.models.resnet18(pretrained=True)\n",
        "input = [torch.rand(128, 3, 224, 224)]   \n",
        "benchmark(test_model, input, 50, 128)\n",
        "\n",
        "def benchmark_torch_function(iters: int, f, *args) -> float:\n",
        "    \"\"\"Estimates the average time duration for a single inference call in second\n",
        "\n",
        "    If the input is batched, then the estimation is for the batches inference call.\n",
        "    \"\"\"\n",
        "    with torch.inference_mode():\n",
        "        f(*args)\n",
        "    torch.cuda.synchronize()\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "    print(\"== Start benchmark iterations\")\n",
        "    with torch.inference_mode():\n",
        "        start_event.record()\n",
        "        for _ in range(iters):\n",
        "            f(*args)\n",
        "        end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"== End benchmark iterations\")\n",
        "    return (start_event.elapsed_time(end_event) * 1.0e-3) / iters\n",
        "\n",
        "\n",
        "def run_configuration_benchmark(\n",
        "    module,\n",
        "    input,\n",
        "    conf: Configuration,\n",
        ") -> Result:\n",
        "    print(f\"=== Running benchmark for: {conf}\", \"green\")\n",
        "    time = -1.0\n",
        "\n",
        "    if conf.fp16:\n",
        "        module = module.half()\n",
        "        input = [i.half() for i in input]\n",
        "\n",
        "    if not conf.trt:\n",
        "        # Run eager mode benchmark\n",
        "        time = benchmark_torch_function(conf.batch_iter, lambda: module(*input))\n",
        "    elif not conf.jit:\n",
        "        # Run lowering eager mode benchmark\n",
        "        lowered_module = lower_to_trt(\n",
        "            module,\n",
        "            input,\n",
        "            max_batch_size=conf.batch_size,\n",
        "            lower_precision=LowerPrecision.FP16 if conf.fp16 else LowerPrecision.FP32,\n",
        "        )\n",
        "        time = benchmark_torch_function(conf.batch_iter, lambda: lowered_module(*input))\n",
        "    else:\n",
        "        print(\"Lowering with JIT is not available!\", \"red\")\n",
        "\n",
        "    result = Result(module=module, input=input, conf=conf, time_sec=time)\n",
        "    return result\n",
        "\n",
        "@torch.inference_mode()\n",
        "def benchmark(\n",
        "    model,\n",
        "    inputs,\n",
        "    batch_iter: int,\n",
        "    batch_size: int,\n",
        ") -> None:\n",
        "    model = model.cuda().eval()\n",
        "    inputs = [x.cuda() for x in inputs]\n",
        "\n",
        "    # benchmark base configuration\n",
        "    conf = Configuration(batch_iter=batch_iter, batch_size=batch_size)\n",
        "\n",
        "    configurations = [\n",
        "        # Baseline\n",
        "        replace(conf, name=\"CUDA Eager\", trt=False),\n",
        "        # FP32\n",
        "        replace(\n",
        "            conf,\n",
        "            name=\"TRT FP32 Eager\",\n",
        "            trt=True,\n",
        "            jit=False,\n",
        "            fp16=False,\n",
        "            accuracy_rtol=1e-3,\n",
        "        ),\n",
        "        # FP16\n",
        "        replace(\n",
        "            conf,\n",
        "            name=\"TRT FP16 Eager\",\n",
        "            trt=True,\n",
        "            jit=False,\n",
        "            fp16=True,\n",
        "            accuracy_rtol=1e-2,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    results = [\n",
        "        run_configuration_benchmark(deepcopy(model), inputs, conf_)\n",
        "        for conf_ in configurations\n",
        "    ]\n",
        "\n",
        "    for res in results:\n",
        "        print(res.format())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running benchmark for: Configuration(batch_iter=50, batch_size=128, name='CUDA Eager', trt=False, jit=False, fp16=False, accuracy_rtol=-1) green\n== Start benchmark iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== End benchmark iterations\n=== Running benchmark for: Configuration(batch_iter=50, batch_size=128, name='TRT FP32 Eager', trt=True, jit=False, fp16=False, accuracy_rtol=0.001) green\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Log pass <function fuse_permute_matmul at 0x7fbdfcc9f1f0> before/after graph to /tmp/tmpaayayg72\n== Log pass <function fuse_permute_linear at 0x7fbe36555f70> before/after graph to /tmp/tmpdw_pq71j\n\nSupported node types in the model:\nacc_ops.conv2d: ((), {'input': torch.float32, 'weight': torch.float32})\nacc_ops.batch_norm: ((), {'input': torch.float32, 'running_mean': torch.float32, 'running_var': torch.float32, 'weight': torch.float32, 'bias': torch.float32})\nacc_ops.relu: ((), {'input': torch.float32})\nacc_ops.max_pool2d: ((), {'input': torch.float32})\nacc_ops.add: ((), {'input': torch.float32, 'other': torch.float32})\nacc_ops.adaptive_avg_pool2d: ((), {'input': torch.float32})\nacc_ops.flatten: ((), {'input': torch.float32})\nacc_ops.linear: ((), {'input': torch.float32, 'weight': torch.float32, 'bias': torch.float32})\n\nUnsupported node types in the model:\n\nGot 1 acc subgraphs and 0 non-acc subgraphs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0627 233146.650 fx2trt.py:190] Run Module elapsed time: 0:00:00.244369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0627 233206.570 fx2trt.py:241] Build TRT engine elapsed time: 0:00:19.918630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Start benchmark iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== End benchmark iterations\n=== Running benchmark for: Configuration(batch_iter=50, batch_size=128, name='TRT FP16 Eager', trt=True, jit=False, fp16=True, accuracy_rtol=0.01) green\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Log pass <function fuse_permute_matmul at 0x7fbdfcc9f1f0> before/after graph to /tmp/tmpnoeblgd5\n== Log pass <function fuse_permute_linear at 0x7fbe36555f70> before/after graph to /tmp/tmpyb1egsof\n\nSupported node types in the model:\nacc_ops.conv2d: ((), {'input': torch.float16, 'weight': torch.float16})\nacc_ops.batch_norm: ((), {'input': torch.float16, 'running_mean': torch.float16, 'running_var': torch.float16, 'weight': torch.float16, 'bias': torch.float16})\nacc_ops.relu: ((), {'input': torch.float16})\nacc_ops.max_pool2d: ((), {'input': torch.float16})\nacc_ops.add: ((), {'input': torch.float16, 'other': torch.float16})\nacc_ops.adaptive_avg_pool2d: ((), {'input': torch.float16})\nacc_ops.flatten: ((), {'input': torch.float16})\nacc_ops.linear: ((), {'input': torch.float16, 'weight': torch.float16, 'bias': torch.float16})\n\nUnsupported node types in the model:\n\nGot 1 acc subgraphs and 0 non-acc subgraphs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0627 233208.996 fx2trt.py:190] Run Module elapsed time: 0:00:00.217076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0627 233244.147 fx2trt.py:241] Build TRT engine elapsed time: 0:00:35.150950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Start benchmark iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== End benchmark iterations\n== Benchmark Result for: Configuration(batch_iter=50, batch_size=128, name='CUDA Eager', trt=False, jit=False, fp16=False, accuracy_rtol=-1)\nBS: 128, Time per iter: 15.00ms, QPS: 8530.72, Accuracy: None (rtol=-1)\n== Benchmark Result for: Configuration(batch_iter=50, batch_size=128, name='TRT FP32 Eager', trt=True, jit=False, fp16=False, accuracy_rtol=0.001)\nBS: 128, Time per iter: 7.95ms, QPS: 16098.45, Accuracy: None (rtol=0.001)\n== Benchmark Result for: Configuration(batch_iter=50, batch_size=128, name='TRT FP16 Eager', trt=True, jit=False, fp16=True, accuracy_rtol=0.01)\nBS: 128, Time per iter: 4.36ms, QPS: 29365.31, Accuracy: None (rtol=0.01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "originalKey": "80bbae99-41ff-4baa-94a5-12bf0c9938f3",
        "showInput": true,
        "customInput": null
      },
      "source": [
        ""
      ]
    }
  ]
}
