{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Torch-TensorRT Getting Started - CitriNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the practice of developing machine learning models, there are few tools as approachable as PyTorch for developing and experimenting in designing machine learning models. The power of PyTorch comes from its deep integration into Python, its flexibility and its approach to automatic differentiation and execution (eager execution). However, when moving from research into production, the requirements change and we may no longer want that deep Python integration and we want optimization to get the best performance we can on our deployment platform. In PyTorch 1.0, TorchScript was introduced as a method to separate your PyTorch model from Python, make it portable and optimizable. TorchScript uses PyTorch's JIT compiler to transform your normal PyTorch code which gets interpreted by the Python interpreter to an intermediate representation (IR) which can have optimizations run on it and at runtime can get interpreted by the PyTorch JIT interpreter. For PyTorch this has opened up a whole new world of possibilities, including deployment in other languages like C++. It also introduces a structured graph based format that we can use to do down to the kernel level optimization of models for inference.\n",
    "\n",
    "When deploying on NVIDIA GPUs TensorRT, NVIDIA's Deep Learning Optimization SDK and Runtime is able to take models from any major framework and specifically tune them to perform better on specific target hardware in the NVIDIA family be it an A100, TITAN V, Jetson Xavier or NVIDIA's Deep Learning Accelerator. TensorRT performs a couple sets of optimizations to achieve this. TensorRT fuses layers and tensors in the model graph, it then uses a large kernel library to select implementations that perform best on the target GPU. TensorRT also has strong support for reduced operating precision execution which allows users to leverage the Tensor Cores on Volta and newer GPUs as well as reducing memory and computation footprints on device.\n",
    "\n",
    "Torch-TensorRT is a compiler that uses TensorRT to optimize TorchScript code, compiling standard TorchScript modules into ones that internally run with TensorRT optimizations. This enables you to continue to remain in the PyTorch ecosystem, using all the great features PyTorch has such as module composability, its flexible tensor implementation, data loaders and more. Torch-TensorRT is available to use with both PyTorch and LibTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates the steps for compiling a TorchScript module with Torch-TensorRT on a pretrained CitriNet network, and running it to test the speedup obtained.\n",
    "\n",
    "## Content\n",
    "1. [Requirements](#1)\n",
    "1. [CitriNet Overview](#2)\n",
    "1. [Creating TorchScript modules](#3)\n",
    "1. [Compiling with Torch-TensorRT](#4)\n",
    "1. [Conclusion](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Requirements\n",
    "\n",
    "Follow the steps in `notebooks/README` to prepare a Docker container, within which you can run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make sure in /CitriNet/perflab the following files exist:\n",
    "    - requirements.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From within CitriNet folder, start docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus all -it --rm -v $PWD:/benchmark --net=host nvcr.io/nvidia/pytorch:21.12-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. From within docker, first install some requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /benchmark\n",
    "!bash ./perflab/requirements.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. CitriNet Overview\n",
    "\n",
    "CitriNet models are end-to-end neural automatic speech recognition (ASR) models that transcribe segments of audio to text.\n",
    "\n",
    "\n",
    "\n",
    "### Model Description\n",
    "\n",
    "Citrinet is a version of [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) that extends [ContextNet](https://arxiv.org/pdf/2005.03191.pdf), utilizing subword encoding (via Word Piece tokenization) and Squeeze-and-Excitation(SE) mechanism and are therefore smaller than QuartzNet models.\n",
    "\n",
    "CitriNet models take in audio segments and transcribe them to letter, byte pair, or word piece sequences. The pretrained models here can be used immediately for fine-tuning or dataset evaluation.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/jasper_vertical.png\" alt=\"alt\" width=\"50%\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and convert Nemo Citrinet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-24 21:59:22 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import torch\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core import typecheck\n",
    "typecheck.set_typecheck_enabled(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving stt_en_citrinet_256...\n",
      "[NeMo I 2022-02-24 21:59:23 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_256/versions/1.0.0rc1/files/stt_en_citrinet_256.nemo to /root/.cache/torch/NeMo/NeMo_1.5.1/stt_en_citrinet_256/91a9cc5850784b2065e8a0aa3d526fd9/stt_en_citrinet_256.nemo\n",
      "100% [..........................................................] 38872168 / 38872168[NeMo I 2022-02-24 21:59:25 common:728] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-02-24 21:59:26 mixins:146] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-24 21:59:26 modelPT:130] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2022-02-24 21:59:26 modelPT:137] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2022-02-24 21:59:26 modelPT:143] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-02-24 21:59:26 features:265] PADDING: 16\n",
      "[NeMo I 2022-02-24 21:59:26 features:282] STFT using torch\n",
      "[NeMo I 2022-02-24 21:59:29 save_restore_connector:149] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_1.5.1/stt_en_citrinet_256/91a9cc5850784b2065e8a0aa3d526fd9/stt_en_citrinet_256.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-02-24 21:59:29 export_utils:198] Swapped 0 modules\n",
      "[NeMo W 2022-02-24 21:59:29 conv_asr:73] Turned off 235 masked convolutions\n",
      "[NeMo W 2022-02-24 21:59:29 export_utils:198] Swapped 0 modules\n",
      "[NeMo W 2022-02-24 21:59:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:916: UserWarning: `optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2022-02-24 21:59:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.loaded_optimizer_states_dict` property is deprecated in v1.4 and will be removed in v1.6.\n",
      "      if hasattr(mod, name):\n",
      "    \n",
      "[NeMo W 2022-02-24 21:59:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "      if hasattr(mod, name):\n",
      "    \n",
      "[NeMo W 2022-02-24 21:59:31 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:669: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "      item = getattr(mod, name)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['stt_en_citrinet_256.ts'],\n",
       " ['nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE exported to ONNX'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions_str = 'fp32'\n",
    "variant = 'stt_en_citrinet_256'\n",
    "batch_sizes = [1, 2, 8]\n",
    "\n",
    "precisions = []\n",
    "if 'fp32' in precisions_str:\n",
    "    precisions.append(torch.float32)\n",
    "if 'fp16' in precisions_str:\n",
    "    precisions.append(torch.half)\n",
    "\n",
    "print(f\"Downloading and saving {variant}...\")\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=variant)\n",
    "asr_model.export(f\"{variant}.ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark utility\n",
    "\n",
    "Let us define a helper function to benchmark a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: stt_en_citrinet_256.ts\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256.ts =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median samples/s: 91.8, mean: 91.6\n",
      "  Median latency (s): 0.010895, mean: 0.010915, 99th_p: 0.011151, std_dev: 0.000068\n",
      "\n",
      "Loading model: stt_en_citrinet_256.ts\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256.ts =================================\n",
      "batch size=8, num iterations=50\n",
      "  Median samples/s: 621.4, mean: 618.7\n",
      "  Median latency (s): 0.012874, mean: 0.012931, 99th_p: 0.013504, std_dev: 0.000155\n",
      "\n",
      "Loading model: stt_en_citrinet_256.ts\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256.ts =================================\n",
      "batch size=32, num iterations=50\n",
      "  Median samples/s: 1196.0, mean: 1194.4\n",
      "  Median latency (s): 0.026755, mean: 0.026795, 99th_p: 0.027511, std_dev: 0.000285\n",
      "\n",
      "Loading model: stt_en_citrinet_256.ts\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256.ts =================================\n",
      "batch size=128, num iterations=50\n",
      "  Median samples/s: 1192.9, mean: 1192.1\n",
      "  Median latency (s): 0.107301, mean: 0.107374, 99th_p: 0.109155, std_dev: 0.000694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_tensorrt as trtorch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def benchmark(model, input_tensor, num_loops, model_name, batch_size):\n",
    "    def timeGraph(model, input_tensor, num_loops):\n",
    "        print(\"Warm up ...\")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(20):\n",
    "                features = model(input_tensor)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Start timing ...\")\n",
    "        timings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_loops):\n",
    "                start_time = timeit.default_timer()\n",
    "                features = model(input_tensor)\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = timeit.default_timer()\n",
    "                timings.append(end_time - start_time)\n",
    "                # print(\"Iteration {}: {:.6f} s\".format(i, end_time - start_time))\n",
    "        return timings\n",
    "    def printStats(graphName, timings, batch_size):\n",
    "        times = np.array(timings)\n",
    "        steps = len(times)\n",
    "        speeds = batch_size / times\n",
    "        time_mean = np.mean(times)\n",
    "        time_med = np.median(times)\n",
    "        time_99th = np.percentile(times, 99)\n",
    "        time_std = np.std(times, ddof=0)\n",
    "        speed_mean = np.mean(speeds)\n",
    "        speed_med = np.median(speeds)\n",
    "        msg = (\"\\n%s =================================\\n\"\n",
    "                \"batch size=%d, num iterations=%d\\n\"\n",
    "                \"  Median samples/s: %.1f, mean: %.1f\\n\"\n",
    "                \"  Median latency (s): %.6f, mean: %.6f, 99th_p: %.6f, std_dev: %.6f\\n\"\n",
    "                ) % (graphName,\n",
    "                    batch_size, steps,\n",
    "                    speed_med, speed_mean,\n",
    "                    time_med, time_mean, time_99th, time_std)\n",
    "        print(msg)\n",
    "    timings = timeGraph(model, input_tensor, num_loops)\n",
    "    printStats(model_name, timings, batch_size)\n",
    "\n",
    "precisions_str = 'fp32' # Precision (default=fp32, fp16)\n",
    "variant = 'stt_en_citrinet_256' # Nemo Citrinet variant\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "trt = False # If True, infer with Torch-TensorRT engine. Else, infer with Pytorch model.\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)\n",
    "    \n",
    "#     timings = timeGraph(model, input_tensor, 50)\n",
    "#     printStats(model_name, timings, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the GPU we are using here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 24 22:00:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Graphics...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| 58%   65C    P0   111W / 200W |   3402MiB / 47681MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Creating TorchScript modules\n",
    "\n",
    "To compile with Torch-TensorRT, the model must first be in **TorchScript**. TorchScript is a programming language included in PyTorch which removes the Python dependency normal PyTorch models have. This conversion is done via a JIT compiler which given a PyTorch Module will generate an equivalent TorchScript Module. There are two paths that can be used to generate TorchScript: **Tracing** and **Scripting**. \n",
    "\n",
    "- Tracing follows execution of PyTorch generating ops in TorchScript corresponding to what it sees. \n",
    "- Scripting does an analysis of the Python code and generates TorchScript, this allows the resulting graph to include control flow which tracing cannot do. \n",
    "\n",
    "Since tracing is yet to be supported in the newest update, we start with an example of the scripted model in TorchScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test0\n",
      "test\n",
      "Generating Torchscript-TensorRT module for batchsize 1 precision torch.float32\n",
      "Generating Torchscript-TensorRT module for batchsize 8 precision torch.float32\n",
      "Generating Torchscript-TensorRT module for batchsize 32 precision torch.float32\n",
      "Generating Torchscript-TensorRT module for batchsize 128 precision torch.float32\n",
      "Generating Torchscript-TensorRT module for batchsize 1 precision torch.float16\n",
      "Generating Torchscript-TensorRT module for batchsize 8 precision torch.float16\n",
      "Generating Torchscript-TensorRT module for batchsize 32 precision torch.float16\n",
      "Generating Torchscript-TensorRT module for batchsize 128 precision torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as trtorch\n",
    "import argparse\n",
    "\n",
    "# trtorch.logging.set_reportable_log_level(trtorch.logging.Level.Info)\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.core import typecheck\n",
    "typecheck.set_typecheck_enabled(False) \n",
    "\n",
    "\n",
    "arg_precisions = \"fp32,fp16\"\n",
    "arg_batch_sizes = \"1,8,32,128\"\n",
    "arg_variant = \"stt_en_citrinet_256\"\n",
    "\n",
    "\n",
    "precisions_str = arg_precisions.split(',')\n",
    "precisions = []\n",
    "if 'fp32' in precisions_str:\n",
    "    precisions.append(torch.float32)\n",
    "if 'fp16' in precisions_str:\n",
    "    precisions.append(torch.half)\n",
    "\n",
    "batch_sizes = [int(x) for x in arg_batch_sizes.split(',')]\n",
    "\n",
    "# print(f\"Downloading and saving {arg_variant}...\")\n",
    "# asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=arg_variant)\n",
    "# asr_model.export(f\"{arg_variant}.ts\")\n",
    "\n",
    "\n",
    "model = torch.jit.load(f\"{arg_variant}.ts\")\n",
    "\n",
    "\n",
    "for precision in precisions:\n",
    "    for batch_size in batch_sizes:\n",
    "        compile_settings = {\n",
    "            \"inputs\": [trtorch.Input(shape=[batch_size, 80, 1488])],\n",
    "            \"enabled_precisions\": {precision},\n",
    "            \"workspace_size\": 2000000000,\n",
    "            \"truncate_long_and_double\": True,\n",
    "        }\n",
    "        print(f\"Generating Torchscript-TensorRT module for batchsize {batch_size} precision {precision}\")\n",
    "        trt_ts_module = trtorch.compile(model, **compile_settings)\n",
    "        torch.jit.save(trt_ts_module, f\"{arg_variant}_bs{batch_size}_{precision}.torch-tensorrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Compiling with Torch-TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript modules behave just like normal PyTorch modules and are intercompatible. From TorchScript we can now compile a TensorRT based module. This module will still be implemented in TorchScript but all the computation will be done in TensorRT.\n",
    "\n",
    "As mentioned earlier, we start with an example of Torch-TensorRT compilation with the traced model.\n",
    "\n",
    "Note that we show benchmarking results of two precisions: FP32 (single precision) and FP16 (half precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 (single precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: stt_en_citrinet_256_bs1_torch.float32.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs1_torch.float32.torch-tensorrt =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median samples/s: 212.2, mean: 255.0\n",
      "  Median latency (s): 0.004713, mean: 0.004136, 99th_p: 0.004853, std_dev: 0.000864\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs8_torch.float32.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs8_torch.float32.torch-tensorrt =================================\n",
      "batch size=8, num iterations=50\n",
      "  Median samples/s: 1607.0, mean: 1591.0\n",
      "  Median latency (s): 0.004978, mean: 0.005186, 99th_p: 0.010805, std_dev: 0.001589\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs32_torch.float32.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs32_torch.float32.torch-tensorrt =================================\n",
      "batch size=32, num iterations=50\n",
      "  Median samples/s: 2309.4, mean: 2275.0\n",
      "  Median latency (s): 0.013857, mean: 0.014101, 99th_p: 0.017073, std_dev: 0.000758\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs128_torch.float32.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs128_torch.float32.torch-tensorrt =================================\n",
      "batch size=128, num iterations=50\n",
      "  Median samples/s: 2582.4, mean: 2575.4\n",
      "  Median latency (s): 0.049566, mean: 0.049717, 99th_p: 0.051990, std_dev: 0.000912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nemo_asr.models.ASRModel.list_available_models()\n",
    "precisions_str = 'fp32' # Precision (default=fp32, fp16)\n",
    "variant = 'stt_en_citrinet_256' # Nemo Citrinet variant\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "trt = True # If True, infer with Torch-TensorRT engine. Else, infer with Pytorch model.\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 (half precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: stt_en_citrinet_256_bs1_torch.float16.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs1_torch.float16.torch-tensorrt =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median samples/s: 281.9, mean: 281.3\n",
      "  Median latency (s): 0.003547, mean: 0.003555, 99th_p: 0.003643, std_dev: 0.000024\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs8_torch.float16.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs8_torch.float16.torch-tensorrt =================================\n",
      "batch size=8, num iterations=50\n",
      "  Median samples/s: 1628.0, mean: 1597.8\n",
      "  Median latency (s): 0.004914, mean: 0.005182, 99th_p: 0.011360, std_dev: 0.001713\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs32_torch.float16.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs32_torch.float16.torch-tensorrt =================================\n",
      "batch size=32, num iterations=50\n",
      "  Median samples/s: 2687.6, mean: 2687.7\n",
      "  Median latency (s): 0.011907, mean: 0.011906, 99th_p: 0.011972, std_dev: 0.000027\n",
      "\n",
      "Loading model: stt_en_citrinet_256_bs128_torch.float16.torch-tensorrt\n",
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "stt_en_citrinet_256_bs128_torch.float16.torch-tensorrt =================================\n",
      "batch size=128, num iterations=50\n",
      "  Median samples/s: 3068.2, mean: 3036.1\n",
      "  Median latency (s): 0.041718, mean: 0.042222, 99th_p: 0.048845, std_dev: 0.001720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precisions_str = 'fp16' # Precision (default=fp32, fp16)\n",
    "variant = 'stt_en_citrinet_256' # Nemo Citrinet variant\n",
    "batch_sizes = [1, 8, 32, 128] # Batch sizes (default=1,8,32,128)\n",
    "trt = True # If True, infer with Torch-TensorRT engine. Else, infer with Pytorch model.\n",
    "precision = torch.float32 if precisions_str =='fp32' else torch.float16\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if trt:\n",
    "        model_name = f\"{variant}_bs{batch_size}_{precision}.torch-tensorrt\"\n",
    "    else:\n",
    "        model_name = f\"{variant}.ts\"\n",
    "\n",
    "    print(f\"Loading model: {model_name}\") \n",
    "    # Load traced model to CPU first\n",
    "    model = torch.jit.load(model_name).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    # Create random input tensor of certain size\n",
    "    torch.manual_seed(12345)\n",
    "    input_shape=(batch_size, 80, 1488)\n",
    "    input_tensor = torch.randn(input_shape).cuda()\n",
    "\n",
    "    # Timing graph inference\n",
    "    benchmark(model, input_tensor, 50, model_name, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we have walked through the complete process of compiling TorchScript models with Torch-TensorRT for CitriNet model and test the performance impact of the optimization. With Torch-TensorRT, we observe a speedup of **2.0X** with FP32, and **2.5X** with FP16.\n",
    "\n",
    "### What's next\n",
    "Now it's time to try Torch-TensorRT on your own model. Fill out issues at https://github.com/NVIDIA/Torch-TensorRT. Your involvement will help future development of Torch-TensorRT.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
